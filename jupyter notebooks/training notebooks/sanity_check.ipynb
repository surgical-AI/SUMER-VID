{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this script seeks to replicate the work of xxx who has explained how to classify video data in his venerable blog post\n",
    "https://medium.com/smileinnovation/training-neural-network-with-image-sequence-an-example-with-video-as-input-c3407f7a0b0f\n",
    "1. Activity recognition\n",
    "\n",
    "\n",
    "hail patrice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path where data lives\n",
    "DATA_PATH = 'human_activity_data/extracted_videos'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/cnn_gru.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import keras\n",
    "# BUG overcome: tensforflow requires visual c++ for some dll file that can be found only there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_video import VideoFrameGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use sub directories names as classes\n",
    "classes = [i.split(os.path.sep)[1] for i in glob.glob('human_activity_data/extracted_videos/*')]\n",
    "classes.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dribble', 'golf', 'kick_ball']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some global params\n",
    "SIZE = (112, 112) # height and width of frame pxl by pxl\n",
    "CHANNELS = 3 # RGB or whatever\n",
    "NBFRAME = 5 # num frames in sequence \n",
    "BS = 8 # Batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pattern to get videos and classes\n",
    "glob_pattern='human_activity_data/extracted_videos/{classname}/*.avi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data augmentation\n",
    "data_aug = keras.preprocessing.image.ImageDataGenerator(\n",
    "    zoom_range=.1,\n",
    "    horizontal_flip=True,\n",
    "    rotation_range=8,\n",
    "    width_shift_range=.2,\n",
    "    height_shift_range=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class dribble, validation count: 47, train count: 98\n",
      "class golf, validation count: 34, train count: 71\n",
      "class kick_ball, validation count: 42, train count: 86\n",
      "Total data: 3 classes for 255 files for train\n"
     ]
    }
   ],
   "source": [
    "# Create video frame generator\n",
    "train = VideoFrameGenerator(\n",
    "    classes=classes, \n",
    "    glob_pattern=glob_pattern,\n",
    "    nb_frames=NBFRAME,\n",
    "    split_val=.33, \n",
    "    shuffle=True,\n",
    "    batch_size=BS,\n",
    "    target_shape=SIZE,\n",
    "    nb_channel=CHANNELS,\n",
    "    transformation=data_aug,\n",
    "    use_frame_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data: 3 classes for 123 files for validation\n"
     ]
    }
   ],
   "source": [
    "# getting validation data\n",
    "valid = train.get_validation_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras_video.utils\n",
    "# keras_video.utils.show_sample(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. A ConvNet is created and Time distributed to detect “features”\n",
    "2. The Time distributed output is injected to GRU or LSTM to treat “time series”\n",
    "3. A DenseNet is then applied to take the decision, to “classify”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUILD CONV NET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, BatchNormalization, \\\n",
    "    MaxPool2D, GlobalMaxPool2D\n",
    "def build_convnet(shape=(112, 112, 3)):\n",
    "    # specify momentum\n",
    "    momentum = .9\n",
    "    # cerate the model from 9=(what is sequential: not clear, just a basic layer stacker class)\n",
    "    model = keras.Sequential()\n",
    "    # builds a 2D convnet, filter #, filter size ...\n",
    "    model.add(Conv2D(64, (3,3), input_shape=shape,\n",
    "        padding='same', activation='relu'))\n",
    "    \n",
    "    # same takes in previous layer, I guess\n",
    "    model.add(Conv2D(64, (3,3), padding='same', activation='relu'))\n",
    "    # adding batch normalization (forgot what batch normalization is used for)\n",
    "    model.add(BatchNormalization(momentum=momentum))\n",
    "    \n",
    "    # max pool to this layer\n",
    "    model.add(MaxPool2D())\n",
    "    \n",
    "    model.add(Conv2D(128, (3,3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(128, (3,3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization(momentum=momentum))\n",
    "    \n",
    "    model.add(MaxPool2D())\n",
    "    \n",
    "    model.add(Conv2D(256, (3,3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(256, (3,3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization(momentum=momentum))\n",
    "    \n",
    "    model.add(MaxPool2D())\n",
    "    \n",
    "    model.add(Conv2D(512, (3,3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(512, (3,3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization(momentum=momentum))\n",
    "    \n",
    "    # flatten... (not sure why not flatten instead of GlobalMaxPool2D)\n",
    "    model.add(GlobalMaxPool2D())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TimeDistributed, GRU, Dense, Dropout\n",
    "# Shape (5, 112, 112, 3) 5 - time sequence length 112x112 = height vs width 3 - num channels\n",
    "def action_model(shape=(5, 112, 112, 3), nbout=3):\n",
    "    # Create our convnet with (112, 112, 3) input shape\n",
    "    convnet = build_convnet(shape[1:])\n",
    "    \n",
    "    # then create our final model\n",
    "    model = keras.Sequential()\n",
    "    # add the convnet with (5, 112, 112, 3) shape\n",
    "    # KEY = allows you to add a time sequence to a layer one at a time\n",
    "    model.add(TimeDistributed(convnet, input_shape=shape))\n",
    "    # here, you can also use GRU or LSTM\n",
    "    model.add(GRU(64))\n",
    "    # and finally, we make a decision network\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(nbout, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## this is where you tell the model how to train - loss function, weight update mechanism\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "INSHAPE=(NBFRAME,) + SIZE + (CHANNELS,) # (5, 112, 112, 3)\n",
    "# what is action model?\n",
    "model = action_model(INSHAPE, len(classes))\n",
    "\n",
    "# this is where you tell the model how to train - loss function, weight update mechanism\n",
    "optimizer = keras.optimizers.Adam(0.001)\n",
    "model.compile(\n",
    "    optimizer,\n",
    "    'categorical_crossentropy',\n",
    "    metrics=['acc']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## epochs, call backs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "31/31 [==============================] - 143s 5s/step - loss: 1.1795 - acc: 0.3550 - val_loss: 1.0161 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00001: saving model to chkp\\weights.01-1.02.hdf5\n",
      "Epoch 2/50\n",
      "31/31 [==============================] - 133s 4s/step - loss: 1.1560 - acc: 0.3511 - val_loss: 0.9906 - val_acc: 0.5333\n",
      "\n",
      "Epoch 00002: saving model to chkp\\weights.02-0.99.hdf5\n",
      "Epoch 3/50\n",
      "31/31 [==============================] - 133s 4s/step - loss: 1.1365 - acc: 0.4197 - val_loss: 0.9303 - val_acc: 0.5917\n",
      "\n",
      "Epoch 00003: saving model to chkp\\weights.03-0.93.hdf5\n",
      "Epoch 4/50\n",
      "31/31 [==============================] - 133s 4s/step - loss: 1.0204 - acc: 0.5169 - val_loss: 0.9534 - val_acc: 0.6083\n",
      "\n",
      "Epoch 00004: saving model to chkp\\weights.04-0.95.hdf5\n",
      "Epoch 5/50\n",
      "31/31 [==============================] - 134s 4s/step - loss: 0.9230 - acc: 0.5848 - val_loss: 1.1104 - val_acc: 0.3917\n",
      "\n",
      "Epoch 00005: saving model to chkp\\weights.05-1.11.hdf5\n",
      "Epoch 6/50\n",
      "31/31 [==============================] - 134s 4s/step - loss: 0.9158 - acc: 0.5568 - val_loss: 1.0251 - val_acc: 0.4667\n",
      "\n",
      "Epoch 00006: saving model to chkp\\weights.06-1.03.hdf5\n",
      "Epoch 7/50\n",
      "31/31 [==============================] - 133s 4s/step - loss: 0.7656 - acc: 0.7012 - val_loss: 0.7019 - val_acc: 0.6583\n",
      "\n",
      "Epoch 00007: saving model to chkp\\weights.07-0.70.hdf5\n",
      "Epoch 8/50\n",
      "31/31 [==============================] - 134s 4s/step - loss: 0.7953 - acc: 0.6825 - val_loss: 0.6694 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00008: saving model to chkp\\weights.08-0.67.hdf5\n",
      "Epoch 9/50\n",
      "31/31 [==============================] - 134s 4s/step - loss: 0.8497 - acc: 0.6939 - val_loss: 0.6055 - val_acc: 0.7833\n",
      "\n",
      "Epoch 00009: saving model to chkp\\weights.09-0.61.hdf5\n",
      "Epoch 10/50\n",
      "31/31 [==============================] - 133s 4s/step - loss: 0.7678 - acc: 0.7135 - val_loss: 1.3933 - val_acc: 0.5083\n",
      "\n",
      "Epoch 00010: saving model to chkp\\weights.10-1.39.hdf5\n",
      "Epoch 11/50\n",
      "31/31 [==============================] - 133s 4s/step - loss: 0.9222 - acc: 0.6535 - val_loss: 1.0686 - val_acc: 0.4333\n",
      "\n",
      "Epoch 00011: saving model to chkp\\weights.11-1.07.hdf5\n",
      "Epoch 12/50\n",
      "31/31 [==============================] - 134s 4s/step - loss: 0.8956 - acc: 0.5377 - val_loss: 0.8499 - val_acc: 0.4833\n",
      "\n",
      "Epoch 00012: saving model to chkp\\weights.12-0.85.hdf5\n",
      "Epoch 13/50\n",
      "31/31 [==============================] - 134s 4s/step - loss: 0.8129 - acc: 0.6394 - val_loss: 0.9388 - val_acc: 0.5250\n",
      "\n",
      "Epoch 00013: saving model to chkp\\weights.13-0.94.hdf5\n",
      "Epoch 14/50\n",
      "31/31 [==============================] - 133s 4s/step - loss: 0.8745 - acc: 0.5725 - val_loss: 0.9226 - val_acc: 0.5167\n",
      "\n",
      "Epoch 00014: saving model to chkp\\weights.14-0.92.hdf5\n",
      "Epoch 15/50\n",
      "31/31 [==============================] - 133s 4s/step - loss: 1.0931 - acc: 0.5061 - val_loss: 0.9465 - val_acc: 0.4417\n",
      "\n",
      "Epoch 00015: saving model to chkp\\weights.15-0.95.hdf5\n",
      "Epoch 16/50\n",
      "31/31 [==============================] - 133s 4s/step - loss: 0.8179 - acc: 0.5486 - val_loss: 0.8332 - val_acc: 0.5250\n",
      "\n",
      "Epoch 00016: saving model to chkp\\weights.16-0.83.hdf5\n",
      "Epoch 17/50\n",
      "31/31 [==============================] - 134s 4s/step - loss: 0.8197 - acc: 0.6230 - val_loss: 0.8601 - val_acc: 0.6500\n",
      "\n",
      "Epoch 00017: saving model to chkp\\weights.17-0.86.hdf5\n",
      "Epoch 18/50\n",
      "31/31 [==============================] - 133s 4s/step - loss: 0.6971 - acc: 0.6935 - val_loss: 0.8853 - val_acc: 0.5250\n",
      "\n",
      "Epoch 00018: saving model to chkp\\weights.18-0.89.hdf5\n",
      "Epoch 19/50\n",
      "31/31 [==============================] - 136s 4s/step - loss: 0.7030 - acc: 0.7076 - val_loss: 0.7191 - val_acc: 0.6250\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00019: saving model to chkp\\weights.19-0.72.hdf5\n",
      "Epoch 20/50\n",
      "31/31 [==============================] - 137s 4s/step - loss: 0.6717 - acc: 0.7232 - val_loss: 0.6156 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00020: saving model to chkp\\weights.20-0.62.hdf5\n",
      "Epoch 21/50\n",
      "31/31 [==============================] - 141s 5s/step - loss: 0.6998 - acc: 0.6845 - val_loss: 0.5709 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00021: saving model to chkp\\weights.21-0.57.hdf5\n",
      "Epoch 22/50\n",
      "31/31 [==============================] - 141s 5s/step - loss: 0.6204 - acc: 0.7284 - val_loss: 0.5559 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00022: saving model to chkp\\weights.22-0.56.hdf5\n",
      "Epoch 23/50\n",
      "31/31 [==============================] - 140s 5s/step - loss: 0.5901 - acc: 0.7427 - val_loss: 0.5226 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00023: saving model to chkp\\weights.23-0.52.hdf5\n",
      "Epoch 24/50\n",
      "31/31 [==============================] - 137s 4s/step - loss: 0.5760 - acc: 0.7793 - val_loss: 0.5174 - val_acc: 0.7750\n",
      "\n",
      "Epoch 00024: saving model to chkp\\weights.24-0.52.hdf5\n",
      "Epoch 25/50\n",
      "31/31 [==============================] - 137s 4s/step - loss: 0.6806 - acc: 0.6736 - val_loss: 0.5353 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00025: saving model to chkp\\weights.25-0.54.hdf5\n",
      "Epoch 26/50\n",
      "31/31 [==============================] - 138s 4s/step - loss: 0.5750 - acc: 0.7730 - val_loss: 0.5370 - val_acc: 0.7750\n",
      "\n",
      "Epoch 00026: saving model to chkp\\weights.26-0.54.hdf5\n",
      "Epoch 27/50\n",
      "31/31 [==============================] - 138s 4s/step - loss: 0.5490 - acc: 0.7549 - val_loss: 0.5014 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00027: saving model to chkp\\weights.27-0.50.hdf5\n",
      "Epoch 28/50\n",
      "31/31 [==============================] - 149s 5s/step - loss: 0.6736 - acc: 0.7360 - val_loss: 0.5533 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00028: saving model to chkp\\weights.28-0.55.hdf5\n",
      "Epoch 29/50\n",
      "31/31 [==============================] - 156s 5s/step - loss: 0.5331 - acc: 0.7733 - val_loss: 0.4965 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00029: saving model to chkp\\weights.29-0.50.hdf5\n",
      "Epoch 30/50\n",
      "31/31 [==============================] - 150s 5s/step - loss: 0.5981 - acc: 0.7806 - val_loss: 0.4993 - val_acc: 0.7833\n",
      "\n",
      "Epoch 00030: saving model to chkp\\weights.30-0.50.hdf5\n",
      "Epoch 31/50\n",
      "31/31 [==============================] - 140s 5s/step - loss: 0.5007 - acc: 0.8181 - val_loss: 0.4305 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00031: saving model to chkp\\weights.31-0.43.hdf5\n",
      "Epoch 32/50\n",
      "31/31 [==============================] - 138s 4s/step - loss: 0.4749 - acc: 0.8122 - val_loss: 0.4777 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00032: saving model to chkp\\weights.32-0.48.hdf5\n",
      "Epoch 33/50\n",
      "31/31 [==============================] - 137s 4s/step - loss: 0.4809 - acc: 0.8096 - val_loss: 0.4263 - val_acc: 0.8417\n",
      "\n",
      "Epoch 00033: saving model to chkp\\weights.33-0.43.hdf5\n",
      "Epoch 34/50\n",
      "31/31 [==============================] - 136s 4s/step - loss: 0.4784 - acc: 0.8005 - val_loss: 0.4200 - val_acc: 0.8250\n",
      "\n",
      "Epoch 00034: saving model to chkp\\weights.34-0.42.hdf5\n",
      "Epoch 35/50\n",
      "31/31 [==============================] - 136s 4s/step - loss: 0.5120 - acc: 0.8086 - val_loss: 0.4466 - val_acc: 0.8417\n",
      "\n",
      "Epoch 00035: saving model to chkp\\weights.35-0.45.hdf5\n",
      "Epoch 36/50\n",
      "31/31 [==============================] - 135s 4s/step - loss: 0.4146 - acc: 0.8347 - val_loss: 0.4460 - val_acc: 0.8250\n",
      "\n",
      "Epoch 00036: saving model to chkp\\weights.36-0.45.hdf5\n",
      "Epoch 37/50\n",
      "31/31 [==============================] - 136s 4s/step - loss: 0.5165 - acc: 0.7776 - val_loss: 0.4967 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00037: saving model to chkp\\weights.37-0.50.hdf5\n",
      "Epoch 38/50\n",
      "31/31 [==============================] - 135s 4s/step - loss: 0.5107 - acc: 0.7760 - val_loss: 0.4424 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00038: saving model to chkp\\weights.38-0.44.hdf5\n",
      "Epoch 39/50\n",
      "31/31 [==============================] - 137s 4s/step - loss: 0.5638 - acc: 0.7557 - val_loss: 0.4150 - val_acc: 0.8667\n",
      "\n",
      "Epoch 00039: saving model to chkp\\weights.39-0.41.hdf5\n",
      "Epoch 40/50\n",
      "31/31 [==============================] - 135s 4s/step - loss: 0.5832 - acc: 0.7687 - val_loss: 0.4507 - val_acc: 0.8667\n",
      "\n",
      "Epoch 00040: saving model to chkp\\weights.40-0.45.hdf5\n",
      "Epoch 41/50\n",
      "31/31 [==============================] - 135s 4s/step - loss: 0.6182 - acc: 0.7445 - val_loss: 0.4016 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00041: saving model to chkp\\weights.41-0.40.hdf5\n",
      "Epoch 42/50\n",
      "31/31 [==============================] - 135s 4s/step - loss: 0.5450 - acc: 0.7652 - val_loss: 0.4475 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00042: saving model to chkp\\weights.42-0.45.hdf5\n",
      "Epoch 43/50\n",
      "31/31 [==============================] - 136s 4s/step - loss: 0.4669 - acc: 0.8335 - val_loss: 0.3646 - val_acc: 0.8833\n",
      "\n",
      "Epoch 00043: saving model to chkp\\weights.43-0.36.hdf5\n",
      "Epoch 44/50\n",
      "31/31 [==============================] - 135s 4s/step - loss: 0.4880 - acc: 0.7975 - val_loss: 0.3959 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00044: saving model to chkp\\weights.44-0.40.hdf5\n",
      "Epoch 45/50\n",
      "31/31 [==============================] - 136s 4s/step - loss: 0.6704 - acc: 0.7663 - val_loss: 0.4105 - val_acc: 0.8417\n",
      "\n",
      "Epoch 00045: saving model to chkp\\weights.45-0.41.hdf5\n",
      "Epoch 46/50\n",
      "31/31 [==============================] - 137s 4s/step - loss: 0.5713 - acc: 0.7600 - val_loss: 0.3849 - val_acc: 0.8833\n",
      "\n",
      "Epoch 00046: saving model to chkp\\weights.46-0.38.hdf5\n",
      "Epoch 47/50\n",
      "31/31 [==============================] - 137s 4s/step - loss: 0.3996 - acc: 0.8278 - val_loss: 0.3282 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00047: saving model to chkp\\weights.47-0.33.hdf5\n",
      "Epoch 48/50\n",
      "31/31 [==============================] - 135s 4s/step - loss: 0.4203 - acc: 0.8267 - val_loss: 0.3538 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00048: saving model to chkp\\weights.48-0.35.hdf5\n",
      "Epoch 49/50\n",
      "31/31 [==============================] - 136s 4s/step - loss: 0.6065 - acc: 0.8007 - val_loss: 0.3925 - val_acc: 0.8667\n",
      "\n",
      "Epoch 00049: saving model to chkp\\weights.49-0.39.hdf5\n",
      "Epoch 50/50\n",
      "31/31 [==============================] - 136s 4s/step - loss: 0.4373 - acc: 0.8242 - val_loss: 0.3712 - val_acc: 0.8667\n",
      "\n",
      "Epoch 00050: saving model to chkp\\weights.50-0.37.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20c0b805760>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS=50\n",
    "# create a \"chkp\" directory before to run that\n",
    "# because ModelCheckpoint will write models inside\n",
    "callbacks = [\n",
    "    keras.callbacks.ReduceLROnPlateau(verbose=1),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        'chkp/weights.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "        verbose=1),\n",
    "]\n",
    "model.fit_generator(\n",
    "    train,\n",
    "    validation_data=valid,\n",
    "    verbose=1,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
