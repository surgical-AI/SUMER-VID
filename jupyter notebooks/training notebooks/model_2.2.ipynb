{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/model_1_outline.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## change log\n",
    "# Epoch to 100 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sequence generator shape (corrected) (b, n, w, h, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changes: \n",
    "### Use camera 2\n",
    "### Trim to 2 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%reload_ext tensorboard\n",
    "import tensorflow as tf\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from patrice's blogpost\n",
    "from keras_video import VideoFrameGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hand_ties', 'suture_throws', 'thread_cuts']\n"
     ]
    }
   ],
   "source": [
    "classes = ['suture_throws', 'hand_ties', 'thread_cuts']\n",
    "classes.sort()\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some global params\n",
    "SIZE = (100, 100) # height and width of frame pxl by pxl\n",
    "CHANNELS = 3 # RGB or whatever\n",
    "NBFRAME = 5 # num frames in sequence \n",
    "BS = 8 # Batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pattern to get videos and classes\n",
    "glob_pattern='../data_v3_model_2.2/model_data/{classname}/*.avi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data augmentation\n",
    "data_aug = keras.preprocessing.image.ImageDataGenerator(\n",
    "    zoom_range=.1,\n",
    "    horizontal_flip=True,\n",
    "    rotation_range=8,\n",
    "    width_shift_range=.2,\n",
    "    height_shift_range=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class hand_ties, validation count: 121, train count: 247\n",
      "class suture_throws, validation count: 122, train count: 248\n",
      "class thread_cuts, validation count: 121, train count: 248\n",
      "Total data: 3 classes for 743 files for train\n"
     ]
    }
   ],
   "source": [
    "# Create video frame generator\n",
    "train = VideoFrameGenerator(\n",
    "    classes=classes, \n",
    "    glob_pattern=glob_pattern,\n",
    "    nb_frames=NBFRAME,\n",
    "    split_val=.33, \n",
    "    shuffle=True,\n",
    "    batch_size=BS,\n",
    "    target_shape=SIZE,\n",
    "    nb_channel=CHANNELS,\n",
    "    use_frame_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data: 3 classes for 364 files for validation\n"
     ]
    }
   ],
   "source": [
    "# getting validation data\n",
    "valid = train.get_validation_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras_video.utils\n",
    "# keras_video.utils.show_sample(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie19.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie285.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie32.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie67.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie235.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie189.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie306.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie81.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie217.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie134.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie115.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie352.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie125.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie218.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie63.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie164.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie208.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie74.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie176.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie287.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie114.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie361.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie146.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie69.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie362.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie310.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie57.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie174.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie98.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie200.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie159.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie14.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie350.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie22.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie333.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie166.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie302.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie293.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie25.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie264.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie112.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie212.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie187.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie130.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie342.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie180.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie137.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie143.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie290.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie358.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie284.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie87.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie65.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie173.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie4.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie233.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie204.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie319.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie149.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie234.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie242.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie56.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie267.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie51.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie304.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie271.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie45.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie356.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie325.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie299.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie131.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie2.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie363.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie357.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie152.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie129.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie172.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie170.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie95.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie346.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie221.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie222.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie10.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie249.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie39.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie351.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie337.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie47.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie11.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie62.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie15.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie349.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie228.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie268.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie332.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie286.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie55.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie281.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie256.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie347.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie7.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie321.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie246.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie96.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie37.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie120.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie261.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie262.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie245.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie72.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie107.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie334.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie320.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie92.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie76.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie17.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie43.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie169.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie54.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie127.avi',\n",
       " '../data_v3_model_2.2/model_data/hand_ties\\\\hand_tie241.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw128.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw326.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw44.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw139.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw49.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw50.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw352.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw198.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw276.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw127.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw236.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw329.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw166.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw114.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw32.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw323.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw62.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw118.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw143.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw278.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw76.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw338.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw223.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw207.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw152.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw335.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw260.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw334.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw187.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw287.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw99.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw7.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw178.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw53.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw111.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw293.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw239.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw151.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw300.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw237.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw56.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw295.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw174.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw82.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw141.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw315.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw35.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw284.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw277.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw31.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw345.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw181.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw149.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw310.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw123.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw270.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw8.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw11.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw172.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw54.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw126.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw294.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw68.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw271.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw246.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw318.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw253.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw240.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw184.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw15.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw131.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw3.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw273.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw215.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw327.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw130.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw19.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw211.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw232.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw40.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw306.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw219.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw337.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw292.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw291.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw159.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw332.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw137.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw347.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw95.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw70.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw58.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw231.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw190.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw83.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw282.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw210.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw356.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw107.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw158.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw309.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw66.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw302.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw362.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw134.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw63.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw202.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw87.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw301.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw65.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw175.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw221.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw14.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw78.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw353.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw242.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw199.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw264.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw171.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw45.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw195.avi',\n",
       " '../data_v3_model_2.2/model_data/suture_throws\\\\suture_throw230.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut367.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut182.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut231.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut295.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut199.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut117.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut188.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut325.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut130.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut264.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut133.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut49.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut276.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut104.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut355.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut210.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut333.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut337.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut61.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut43.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut64.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut273.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut76.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut157.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut350.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut344.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut154.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut206.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut222.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut335.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut317.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut277.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut189.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut205.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut138.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut129.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut158.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut153.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut260.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut364.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut197.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut58.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut310.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut92.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut326.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut280.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut305.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut238.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut12.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut236.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut106.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut148.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut248.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut18.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut311.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut60.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut165.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut126.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut168.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut63.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut261.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut62.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut121.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut75.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut226.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut161.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut38.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut78.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut77.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut322.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut291.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut162.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut345.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut140.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut241.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut54.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut245.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut272.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut249.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut327.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut108.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut250.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut290.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut95.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut288.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut228.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut201.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut23.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut192.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut25.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut243.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut300.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut96.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut218.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut84.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut208.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut119.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut99.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut219.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut242.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut302.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut196.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut369.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut314.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut42.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut171.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut316.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut93.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut144.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut180.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut271.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut72.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut270.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut315.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut220.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut211.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut179.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut284.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut174.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut331.avi',\n",
       " '../data_v3_model_2.2/model_data/thread_cuts\\\\thread_cut143.avi']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid.files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUILD CONV NET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, BatchNormalization, \\\n",
    "    MaxPool2D, GlobalMaxPool2D\n",
    "def build_mobilenet(shape=(224, 224, 3), nbout=3):\n",
    "    model = keras.applications.mobilenet.MobileNet(\n",
    "        include_top=False,\n",
    "        input_shape=shape,\n",
    "        weights='imagenet')\n",
    "    # Keep 9 layers to train﻿﻿\n",
    "    trainable = 9\n",
    "    for layer in model.layers[:-trainable]:\n",
    "        layer.trainable = False\n",
    "    for layer in model.layers[-trainable:]:\n",
    "        layer.trainable = True\n",
    "    output = GlobalMaxPool2D()\n",
    "    return keras.Sequential([model, output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TimeDistributed, GRU, Dense, Dropout\n",
    "# Shape (5, 112, 112, 3) 5 - time sequence length 112x112 = height vs width 3 - num channels\n",
    "def action_model(shape=(5, 112, 112, 3), nbout=3):\n",
    "    # Create our convnet with (112, 112, 3) input shape\n",
    "    convnet = build_mobilenet(shape[1:])\n",
    "    \n",
    "    # then create our final model\n",
    "    model = keras.Sequential()\n",
    "    # add the convnet with (5, 112, 112, 3) shape\n",
    "    # KEY = allows you to add a time sequence to a layer one at a time\n",
    "    model.add(TimeDistributed(convnet, input_shape=shape))\n",
    "    # here, you can also use GRU or LSTM\n",
    "    model.add(GRU(64))\n",
    "    # and finally, we make a decision network\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(nbout, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## this is where you tell the model how to train - loss function, weight update mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "INSHAPE=(NBFRAME,) + SIZE + (CHANNELS,) # (5, 112, 112, 3)\n",
    "# action model - GRU set up for Time shifted CNN\n",
    "model = action_model(INSHAPE, len(classes))\n",
    "\n",
    "# this is where you tell the model how to train - loss function, weight update mechanism\n",
    "optimizer = keras.optimizers.Adam(0.001)\n",
    "model.compile(\n",
    "    optimizer,\n",
    "    'categorical_crossentropy',\n",
    "    metrics=['acc']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## epochs, call backs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs for this run are here: logs/fit/20210406-144120\n"
     ]
    }
   ],
   "source": [
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "print('logs for this run are here: {}'.format(log_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "92/92 [==============================] - 364s 4s/step - loss: 1.0992 - acc: 0.3971 - val_loss: 1.0063 - val_acc: 0.5361\n",
      "\n",
      "Epoch 00001: saving model to model2_2_chkp\\weights.01-1.01.hdf5\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 27s 290ms/step - loss: 0.9060 - acc: 0.5748 - val_loss: 0.8158 - val_acc: 0.6222\n",
      "\n",
      "Epoch 00002: saving model to model2_2_chkp\\weights.02-0.82.hdf5\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 24s 260ms/step - loss: 0.7791 - acc: 0.6531 - val_loss: 0.8065 - val_acc: 0.6250\n",
      "\n",
      "Epoch 00003: saving model to model2_2_chkp\\weights.03-0.81.hdf5\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 24s 261ms/step - loss: 0.7672 - acc: 0.6497 - val_loss: 0.6254 - val_acc: 0.7861\n",
      "\n",
      "Epoch 00004: saving model to model2_2_chkp\\weights.04-0.63.hdf5\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 25s 271ms/step - loss: 0.5971 - acc: 0.7656 - val_loss: 0.6518 - val_acc: 0.7389\n",
      "\n",
      "Epoch 00005: saving model to model2_2_chkp\\weights.05-0.65.hdf5\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 25s 271ms/step - loss: 0.4670 - acc: 0.8302 - val_loss: 0.5309 - val_acc: 0.7694\n",
      "\n",
      "Epoch 00006: saving model to model2_2_chkp\\weights.06-0.53.hdf5\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 25s 269ms/step - loss: 0.5057 - acc: 0.8321 - val_loss: 0.4784 - val_acc: 0.8139\n",
      "\n",
      "Epoch 00007: saving model to model2_2_chkp\\weights.07-0.48.hdf5\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 24s 266ms/step - loss: 0.4005 - acc: 0.8555 - val_loss: 0.6246 - val_acc: 0.7306\n",
      "\n",
      "Epoch 00008: saving model to model2_2_chkp\\weights.08-0.62.hdf5\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 24s 261ms/step - loss: 0.3850 - acc: 0.8725 - val_loss: 0.4608 - val_acc: 0.8222\n",
      "\n",
      "Epoch 00009: saving model to model2_2_chkp\\weights.09-0.46.hdf5\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 25s 270ms/step - loss: 0.3639 - acc: 0.8746 - val_loss: 0.6343 - val_acc: 0.7833\n",
      "\n",
      "Epoch 00010: saving model to model2_2_chkp\\weights.10-0.63.hdf5\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 25s 271ms/step - loss: 0.3222 - acc: 0.8948 - val_loss: 0.5303 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00011: saving model to model2_2_chkp\\weights.11-0.53.hdf5\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 25s 268ms/step - loss: 0.2951 - acc: 0.8957 - val_loss: 0.4367 - val_acc: 0.8222\n",
      "\n",
      "Epoch 00012: saving model to model2_2_chkp\\weights.12-0.44.hdf5\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 25s 268ms/step - loss: 0.2934 - acc: 0.9072 - val_loss: 0.6320 - val_acc: 0.7583\n",
      "\n",
      "Epoch 00013: saving model to model2_2_chkp\\weights.13-0.63.hdf5\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 24s 263ms/step - loss: 0.3537 - acc: 0.8820 - val_loss: 0.4789 - val_acc: 0.8694\n",
      "\n",
      "Epoch 00014: saving model to model2_2_chkp\\weights.14-0.48.hdf5\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 25s 270ms/step - loss: 0.2360 - acc: 0.9277 - val_loss: 0.3380 - val_acc: 0.8833\n",
      "\n",
      "Epoch 00015: saving model to model2_2_chkp\\weights.15-0.34.hdf5\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 25s 270ms/step - loss: 0.2883 - acc: 0.9194 - val_loss: 0.3336 - val_acc: 0.8889\n",
      "\n",
      "Epoch 00016: saving model to model2_2_chkp\\weights.16-0.33.hdf5\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 25s 268ms/step - loss: 0.1555 - acc: 0.9523 - val_loss: 0.4736 - val_acc: 0.8611\n",
      "\n",
      "Epoch 00017: saving model to model2_2_chkp\\weights.17-0.47.hdf5\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 25s 272ms/step - loss: 0.1974 - acc: 0.9455 - val_loss: 0.4788 - val_acc: 0.8583\n",
      "\n",
      "Epoch 00018: saving model to model2_2_chkp\\weights.18-0.48.hdf5\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 25s 270ms/step - loss: 0.2257 - acc: 0.9359 - val_loss: 0.6716 - val_acc: 0.7944\n",
      "\n",
      "Epoch 00019: saving model to model2_2_chkp\\weights.19-0.67.hdf5\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 25s 271ms/step - loss: 0.2902 - acc: 0.9141 - val_loss: 0.4047 - val_acc: 0.8694\n",
      "\n",
      "Epoch 00020: saving model to model2_2_chkp\\weights.20-0.40.hdf5\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 25s 270ms/step - loss: 0.1206 - acc: 0.9654 - val_loss: 0.3322 - val_acc: 0.8917\n",
      "\n",
      "Epoch 00021: saving model to model2_2_chkp\\weights.21-0.33.hdf5\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 24s 260ms/step - loss: 0.2545 - acc: 0.9128 - val_loss: 0.4494 - val_acc: 0.8778\n",
      "\n",
      "Epoch 00022: saving model to model2_2_chkp\\weights.22-0.45.hdf5\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 25s 269ms/step - loss: 0.1392 - acc: 0.9551 - val_loss: 0.4894 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00023: saving model to model2_2_chkp\\weights.23-0.49.hdf5\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 25s 271ms/step - loss: 0.1724 - acc: 0.9495 - val_loss: 0.4261 - val_acc: 0.8694\n",
      "\n",
      "Epoch 00024: saving model to model2_2_chkp\\weights.24-0.43.hdf5\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 25s 272ms/step - loss: 0.1867 - acc: 0.9467 - val_loss: 0.7073 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00025: saving model to model2_2_chkp\\weights.25-0.71.hdf5\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 25s 274ms/step - loss: 0.2517 - acc: 0.9216 - val_loss: 0.5439 - val_acc: 0.8556\n",
      "\n",
      "Epoch 00026: saving model to model2_2_chkp\\weights.26-0.54.hdf5\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 25s 269ms/step - loss: 0.1497 - acc: 0.9685 - val_loss: 0.7550 - val_acc: 0.7722\n",
      "\n",
      "Epoch 00027: saving model to model2_2_chkp\\weights.27-0.76.hdf5\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 24s 265ms/step - loss: 0.2456 - acc: 0.9355 - val_loss: 0.9845 - val_acc: 0.6389\n",
      "\n",
      "Epoch 00028: saving model to model2_2_chkp\\weights.28-0.98.hdf5\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 24s 259ms/step - loss: 0.2751 - acc: 0.9398 - val_loss: 0.8893 - val_acc: 0.6556\n",
      "\n",
      "Epoch 00029: saving model to model2_2_chkp\\weights.29-0.89.hdf5\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 25s 273ms/step - loss: 0.2895 - acc: 0.9025 - val_loss: 0.4837 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00030: saving model to model2_2_chkp\\weights.30-0.48.hdf5\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 25s 270ms/step - loss: 0.2019 - acc: 0.9414 - val_loss: 0.4718 - val_acc: 0.8611\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00031: saving model to model2_2_chkp\\weights.31-0.47.hdf5\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 25s 269ms/step - loss: 0.1750 - acc: 0.9452 - val_loss: 0.4230 - val_acc: 0.8806\n",
      "\n",
      "Epoch 00032: saving model to model2_2_chkp\\weights.32-0.42.hdf5\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 25s 267ms/step - loss: 0.1789 - acc: 0.9456 - val_loss: 0.4055 - val_acc: 0.8833\n",
      "\n",
      "Epoch 00033: saving model to model2_2_chkp\\weights.33-0.41.hdf5\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 25s 268ms/step - loss: 0.1682 - acc: 0.9421 - val_loss: 0.3749 - val_acc: 0.9028\n",
      "\n",
      "Epoch 00034: saving model to model2_2_chkp\\weights.34-0.37.hdf5\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 25s 268ms/step - loss: 0.1437 - acc: 0.9519 - val_loss: 0.3536 - val_acc: 0.8944\n",
      "\n",
      "Epoch 00035: saving model to model2_2_chkp\\weights.35-0.35.hdf5\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 25s 267ms/step - loss: 0.1320 - acc: 0.9624 - val_loss: 0.3364 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00036: saving model to model2_2_chkp\\weights.36-0.34.hdf5\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 25s 268ms/step - loss: 0.0859 - acc: 0.9730 - val_loss: 0.3466 - val_acc: 0.9194\n",
      "\n",
      "Epoch 00037: saving model to model2_2_chkp\\weights.37-0.35.hdf5\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 24s 265ms/step - loss: 0.1303 - acc: 0.9646 - val_loss: 0.3534 - val_acc: 0.9139\n",
      "\n",
      "Epoch 00038: saving model to model2_2_chkp\\weights.38-0.35.hdf5\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 25s 268ms/step - loss: 0.1446 - acc: 0.9537 - val_loss: 0.3223 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00039: saving model to model2_2_chkp\\weights.39-0.32.hdf5\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 25s 269ms/step - loss: 0.0988 - acc: 0.9721 - val_loss: 0.3514 - val_acc: 0.9222\n",
      "\n",
      "Epoch 00040: saving model to model2_2_chkp\\weights.40-0.35.hdf5\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 25s 268ms/step - loss: 0.0638 - acc: 0.9790 - val_loss: 0.3577 - val_acc: 0.9167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00041: saving model to model2_2_chkp\\weights.41-0.36.hdf5\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 24s 262ms/step - loss: 0.0549 - acc: 0.9890 - val_loss: 0.3746 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00042: saving model to model2_2_chkp\\weights.42-0.37.hdf5\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 25s 270ms/step - loss: 0.0638 - acc: 0.9837 - val_loss: 0.4040 - val_acc: 0.9139\n",
      "\n",
      "Epoch 00043: saving model to model2_2_chkp\\weights.43-0.40.hdf5\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 24s 266ms/step - loss: 0.0598 - acc: 0.9823 - val_loss: 0.4397 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00044: saving model to model2_2_chkp\\weights.44-0.44.hdf5\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 25s 267ms/step - loss: 0.0670 - acc: 0.9762 - val_loss: 0.4083 - val_acc: 0.9083\n",
      "\n",
      "Epoch 00045: saving model to model2_2_chkp\\weights.45-0.41.hdf5\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 25s 270ms/step - loss: 0.1031 - acc: 0.9686 - val_loss: 0.3517 - val_acc: 0.9083\n",
      "\n",
      "Epoch 00046: saving model to model2_2_chkp\\weights.46-0.35.hdf5\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 24s 266ms/step - loss: 0.0514 - acc: 0.9897 - val_loss: 0.3758 - val_acc: 0.9083\n",
      "\n",
      "Epoch 00047: saving model to model2_2_chkp\\weights.47-0.38.hdf5\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 24s 261ms/step - loss: 0.0553 - acc: 0.9840 - val_loss: 0.3865 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00048: saving model to model2_2_chkp\\weights.48-0.39.hdf5\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 25s 270ms/step - loss: 0.0352 - acc: 0.9884 - val_loss: 0.4039 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00049: saving model to model2_2_chkp\\weights.49-0.40.hdf5\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 24s 264ms/step - loss: 0.0597 - acc: 0.9819 - val_loss: 0.3853 - val_acc: 0.9194\n",
      "\n",
      "Epoch 00050: saving model to model2_2_chkp\\weights.50-0.39.hdf5\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 25s 270ms/step - loss: 0.0758 - acc: 0.9753 - val_loss: 0.4027 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00051: saving model to model2_2_chkp\\weights.51-0.40.hdf5\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 25s 272ms/step - loss: 0.0409 - acc: 0.9872 - val_loss: 0.3963 - val_acc: 0.9139\n",
      "\n",
      "Epoch 00052: saving model to model2_2_chkp\\weights.52-0.40.hdf5\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 25s 270ms/step - loss: 0.0829 - acc: 0.9722 - val_loss: 0.3961 - val_acc: 0.9194\n",
      "\n",
      "Epoch 00053: saving model to model2_2_chkp\\weights.53-0.40.hdf5\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 25s 267ms/step - loss: 0.0527 - acc: 0.9843 - val_loss: 0.3933 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00054: saving model to model2_2_chkp\\weights.54-0.39.hdf5\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 24s 262ms/step - loss: 0.0286 - acc: 0.9901 - val_loss: 0.3979 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00055: saving model to model2_2_chkp\\weights.55-0.40.hdf5\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 25s 270ms/step - loss: 0.1014 - acc: 0.9680 - val_loss: 0.3740 - val_acc: 0.9194\n",
      "\n",
      "Epoch 00056: saving model to model2_2_chkp\\weights.56-0.37.hdf5\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 25s 267ms/step - loss: 0.0407 - acc: 0.9854 - val_loss: 0.3899 - val_acc: 0.9194\n",
      "\n",
      "Epoch 00057: saving model to model2_2_chkp\\weights.57-0.39.hdf5\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 25s 268ms/step - loss: 0.0625 - acc: 0.9872 - val_loss: 0.3943 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00058: saving model to model2_2_chkp\\weights.58-0.39.hdf5\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 25s 267ms/step - loss: 0.0476 - acc: 0.9880 - val_loss: 0.3739 - val_acc: 0.9222\n",
      "\n",
      "Epoch 00059: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00059: saving model to model2_2_chkp\\weights.59-0.37.hdf5\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 24s 259ms/step - loss: 0.0304 - acc: 0.9911 - val_loss: 0.3839 - val_acc: 0.9194\n",
      "\n",
      "Epoch 00060: saving model to model2_2_chkp\\weights.60-0.38.hdf5\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 24s 265ms/step - loss: 0.0283 - acc: 0.9898 - val_loss: 0.3826 - val_acc: 0.9194\n",
      "\n",
      "Epoch 00061: saving model to model2_2_chkp\\weights.61-0.38.hdf5\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 25s 270ms/step - loss: 0.0414 - acc: 0.9904 - val_loss: 0.3760 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00062: saving model to model2_2_chkp\\weights.62-0.38.hdf5\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 26s 278ms/step - loss: 0.0492 - acc: 0.9888 - val_loss: 0.4057 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00063: saving model to model2_2_chkp\\weights.63-0.41.hdf5\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 25s 269ms/step - loss: 0.0472 - acc: 0.9847 - val_loss: 0.3886 - val_acc: 0.9194\n",
      "\n",
      "Epoch 00064: saving model to model2_2_chkp\\weights.64-0.39.hdf5\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 25s 269ms/step - loss: 0.0361 - acc: 0.9848 - val_loss: 0.3981 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00065: saving model to model2_2_chkp\\weights.65-0.40.hdf5\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 25s 267ms/step - loss: 0.0428 - acc: 0.9896 - val_loss: 0.3957 - val_acc: 0.9139\n",
      "\n",
      "Epoch 00066: saving model to model2_2_chkp\\weights.66-0.40.hdf5\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 24s 259ms/step - loss: 0.0375 - acc: 0.9887 - val_loss: 0.3979 - val_acc: 0.9139\n",
      "\n",
      "Epoch 00067: saving model to model2_2_chkp\\weights.67-0.40.hdf5\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 25s 274ms/step - loss: 0.0415 - acc: 0.9866 - val_loss: 0.3995 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00068: saving model to model2_2_chkp\\weights.68-0.40.hdf5\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 25s 271ms/step - loss: 0.0458 - acc: 0.9823 - val_loss: 0.3717 - val_acc: 0.9222\n",
      "\n",
      "Epoch 00069: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "\n",
      "Epoch 00069: saving model to model2_2_chkp\\weights.69-0.37.hdf5\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 25s 271ms/step - loss: 0.0632 - acc: 0.9798 - val_loss: 0.4078 - val_acc: 0.9139\n",
      "\n",
      "Epoch 00070: saving model to model2_2_chkp\\weights.70-0.41.hdf5\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 24s 262ms/step - loss: 0.0413 - acc: 0.9836 - val_loss: 0.3918 - val_acc: 0.9194\n",
      "\n",
      "Epoch 00071: saving model to model2_2_chkp\\weights.71-0.39.hdf5\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 25s 270ms/step - loss: 0.1144 - acc: 0.9738 - val_loss: 0.3974 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00072: saving model to model2_2_chkp\\weights.72-0.40.hdf5\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 25s 272ms/step - loss: 0.0487 - acc: 0.9884 - val_loss: 0.4003 - val_acc: 0.9139\n",
      "\n",
      "Epoch 00073: saving model to model2_2_chkp\\weights.73-0.40.hdf5\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 24s 266ms/step - loss: 0.0604 - acc: 0.9760 - val_loss: 0.4029 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00074: saving model to model2_2_chkp\\weights.74-0.40.hdf5\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 25s 269ms/step - loss: 0.0288 - acc: 0.9888 - val_loss: 0.4023 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00075: saving model to model2_2_chkp\\weights.75-0.40.hdf5\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 25s 272ms/step - loss: 0.0533 - acc: 0.9782 - val_loss: 0.4032 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00076: saving model to model2_2_chkp\\weights.76-0.40.hdf5\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 25s 269ms/step - loss: 0.0379 - acc: 0.9900 - val_loss: 0.3983 - val_acc: 0.9139\n",
      "\n",
      "Epoch 00077: saving model to model2_2_chkp\\weights.77-0.40.hdf5\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 25s 271ms/step - loss: 0.0324 - acc: 0.9928 - val_loss: 0.3938 - val_acc: 0.9139\n",
      "\n",
      "Epoch 00078: saving model to model2_2_chkp\\weights.78-0.39.hdf5\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 25s 268ms/step - loss: 0.0893 - acc: 0.9854 - val_loss: 0.4029 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00079: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "\n",
      "Epoch 00079: saving model to model2_2_chkp\\weights.79-0.40.hdf5\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 24s 263ms/step - loss: 0.0431 - acc: 0.9878 - val_loss: 0.3905 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00080: saving model to model2_2_chkp\\weights.80-0.39.hdf5\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92/92 [==============================] - 24s 267ms/step - loss: 0.0601 - acc: 0.9784 - val_loss: 0.4022 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00081: saving model to model2_2_chkp\\weights.81-0.40.hdf5\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 25s 269ms/step - loss: 0.0262 - acc: 0.9934 - val_loss: 0.3980 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00082: saving model to model2_2_chkp\\weights.82-0.40.hdf5\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 25s 268ms/step - loss: 0.0295 - acc: 0.9896 - val_loss: 0.3817 - val_acc: 0.9222\n",
      "\n",
      "Epoch 00083: saving model to model2_2_chkp\\weights.83-0.38.hdf5\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 24s 262ms/step - loss: 0.0266 - acc: 0.9900 - val_loss: 0.3927 - val_acc: 0.9194\n",
      "\n",
      "Epoch 00084: saving model to model2_2_chkp\\weights.84-0.39.hdf5\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 24s 266ms/step - loss: 0.0852 - acc: 0.9657 - val_loss: 0.3977 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00085: saving model to model2_2_chkp\\weights.85-0.40.hdf5\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 25s 270ms/step - loss: 0.0359 - acc: 0.9876 - val_loss: 0.3912 - val_acc: 0.9139\n",
      "\n",
      "Epoch 00086: saving model to model2_2_chkp\\weights.86-0.39.hdf5\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 24s 266ms/step - loss: 0.0544 - acc: 0.9871 - val_loss: 0.3859 - val_acc: 0.9194\n",
      "\n",
      "Epoch 00087: saving model to model2_2_chkp\\weights.87-0.39.hdf5\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 25s 269ms/step - loss: 0.0410 - acc: 0.9886 - val_loss: 0.3959 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00088: saving model to model2_2_chkp\\weights.88-0.40.hdf5\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 24s 262ms/step - loss: 0.0417 - acc: 0.9831 - val_loss: 0.4031 - val_acc: 0.9111\n",
      "\n",
      "Epoch 00089: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "\n",
      "Epoch 00089: saving model to model2_2_chkp\\weights.89-0.40.hdf5\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 25s 271ms/step - loss: 0.0410 - acc: 0.9892 - val_loss: 0.3831 - val_acc: 0.9194\n",
      "\n",
      "Epoch 00090: saving model to model2_2_chkp\\weights.90-0.38.hdf5\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 25s 270ms/step - loss: 0.0443 - acc: 0.9847 - val_loss: 0.3991 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00091: saving model to model2_2_chkp\\weights.91-0.40.hdf5\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 25s 270ms/step - loss: 0.0556 - acc: 0.9865 - val_loss: 0.3998 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00092: saving model to model2_2_chkp\\weights.92-0.40.hdf5\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 24s 266ms/step - loss: 0.0732 - acc: 0.9823 - val_loss: 0.4017 - val_acc: 0.9111\n",
      "\n",
      "Epoch 00093: saving model to model2_2_chkp\\weights.93-0.40.hdf5\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 24s 263ms/step - loss: 0.0574 - acc: 0.9873 - val_loss: 0.3884 - val_acc: 0.9194\n",
      "\n",
      "Epoch 00094: saving model to model2_2_chkp\\weights.94-0.39.hdf5\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 25s 267ms/step - loss: 0.0488 - acc: 0.9850 - val_loss: 0.3959 - val_acc: 0.9194\n",
      "\n",
      "Epoch 00095: saving model to model2_2_chkp\\weights.95-0.40.hdf5\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 25s 271ms/step - loss: 0.0325 - acc: 0.9915 - val_loss: 0.4022 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00096: saving model to model2_2_chkp\\weights.96-0.40.hdf5\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 24s 267ms/step - loss: 0.0356 - acc: 0.9894 - val_loss: 0.3986 - val_acc: 0.9139\n",
      "\n",
      "Epoch 00097: saving model to model2_2_chkp\\weights.97-0.40.hdf5\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 24s 262ms/step - loss: 0.0830 - acc: 0.9733 - val_loss: 0.4055 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00098: saving model to model2_2_chkp\\weights.98-0.41.hdf5\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 25s 271ms/step - loss: 0.0392 - acc: 0.9881 - val_loss: 0.4004 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00099: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-10.\n",
      "\n",
      "Epoch 00099: saving model to model2_2_chkp\\weights.99-0.40.hdf5\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 25s 270ms/step - loss: 0.0423 - acc: 0.9882 - val_loss: 0.3889 - val_acc: 0.9194\n",
      "\n",
      "Epoch 00100: saving model to model2_2_chkp\\weights.100-0.39.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x11a49c4f640>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS=100\n",
    "# create a \"chkp\" directory before to run that\n",
    "# because ModelCheckpoint will write models inside\n",
    "callbacks = [\n",
    "    keras.callbacks.ReduceLROnPlateau(verbose=1),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        'model2_2_chkp/weights.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "        verbose=1),\n",
    "    tensorboard_callback\n",
    "]\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "model.fit(\n",
    "    train,\n",
    "    validation_data=valid,\n",
    "    verbose=1,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
